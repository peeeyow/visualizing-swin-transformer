{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Swin Transformer\n",
    "\n",
    "**by Pio Lauren T. Mendoza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"image_net_1k_labels.txt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "labels = [label.strip(\"\"\"'\"\\n\"\"\") for label in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pio/workfiles/CoE197Z/visualizing-swin-transformer/venv/lib64/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
    "model.eval()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL '': No schema supplied. Perhaps you meant http://?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_126908/4292235568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfiles/CoE197Z/visualizing-swin-transformer/venv/lib64/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfiles/CoE197Z/visualizing-swin-transformer/venv/lib64/python3.9/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfiles/CoE197Z/visualizing-swin-transformer/venv/lib64/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfiles/CoE197Z/visualizing-swin-transformer/venv/lib64/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         p.prepare(\n\u001b[0m\u001b[1;32m    457\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfiles/CoE197Z/visualizing-swin-transformer/venv/lib64/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workfiles/CoE197Z/visualizing-swin-transformer/venv/lib64/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '': No schema supplied. Perhaps you meant http://?"
     ]
    }
   ],
   "source": [
    "img = Image.open(requests.get(url, stream=True).raw).resize((224,224)).convert('RGB')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tens = transform(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "  output = model(img_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'racer, race car, racing car'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[output.max(-1).indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "SwinTransformer                                    --\n",
       "├─PatchEmbed: 1-1                                  --\n",
       "│    └─Conv2d: 2-1                                 6,272\n",
       "│    └─LayerNorm: 2-2                              256\n",
       "├─Dropout: 1-2                                     --\n",
       "├─Sequential: 1-3                                  --\n",
       "│    └─BasicLayer: 2-3                             --\n",
       "│    │    └─ModuleList: 3-1                        397,896\n",
       "│    │    └─PatchMerging: 3-2                      132,096\n",
       "│    └─BasicLayer: 2-4                             --\n",
       "│    │    └─ModuleList: 3-3                        1,582,224\n",
       "│    │    └─PatchMerging: 3-4                      526,336\n",
       "│    └─BasicLayer: 2-5                             --\n",
       "│    │    └─ModuleList: 3-5                        56,791,584\n",
       "│    │    └─PatchMerging: 3-6                      2,101,248\n",
       "│    └─BasicLayer: 2-6                             --\n",
       "│    │    └─ModuleList: 3-7                        25,203,264\n",
       "├─LayerNorm: 1-4                                   2,048\n",
       "├─AdaptiveAvgPool1d: 1-5                           --\n",
       "├─Linear: 1-6                                      1,025,000\n",
       "===========================================================================\n",
       "Total params: 87,704,680\n",
       "Trainable params: 87,704,680\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PatchEmbed(\n",
       "   (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "   (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Sequential(\n",
       "   (0): BasicLayer(\n",
       "     dim=128, input_resolution=(56, 56), depth=2\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): Identity()\n",
       "         (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (downsample): PatchMerging(\n",
       "       input_resolution=(56, 56), dim=128\n",
       "       (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "       (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicLayer(\n",
       "     dim=256, input_resolution=(28, 28), depth=2\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (downsample): PatchMerging(\n",
       "       input_resolution=(28, 28), dim=256\n",
       "       (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "       (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (2): BasicLayer(\n",
       "     dim=512, input_resolution=(14, 14), depth=18\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (2): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (4): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (5): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (6): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (7): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (8): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (9): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (10): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (11): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (12): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (13): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (14): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (15): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (16): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (17): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (downsample): PatchMerging(\n",
       "       input_resolution=(14, 14), dim=512\n",
       "       (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "       (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (3): BasicLayer(\n",
       "     dim=1024, input_resolution=(7, 7), depth=2\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1024,), eps=1e-05, elementwise_affine=True),\n",
       " AdaptiveAvgPool1d(output_size=1),\n",
       " Linear(in_features=1024, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b6b354958d59b4890170593aabca2d81d77c2a3806ff7f39955507ba6dfc558"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
