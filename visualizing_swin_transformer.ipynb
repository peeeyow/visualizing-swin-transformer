{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Swin Transformer\n",
    "\n",
    "**by Pio Lauren T. Mendoza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "The blackcellmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext blackcellmagic\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from torchinfo import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"imagenet_classes.txt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "labels = [label.strip(\"\"\"'\"\\n\"\"\") for label in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19478/3523167087.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = T.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = input()\n",
    "img = Image.open(requests.get(url, stream=True).raw).resize((224,224)).convert('RGB')\n",
    "img_tens = transform(img).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "  output = model(img_tens)\n",
    "display(img)\n",
    "labels[output.max(-1).indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "SwinTransformer                                    --                        --\n",
       "├─Sequential: 1                                    --                        --\n",
       "│    └─BasicLayer: 2                               --                        --\n",
       "│    │    └─ModuleList: 3-1                        --                        397,896\n",
       "│    └─BasicLayer: 2                               --                        --\n",
       "│    │    └─ModuleList: 3-2                        --                        1,582,224\n",
       "│    └─BasicLayer: 2                               --                        --\n",
       "│    │    └─ModuleList: 3-3                        --                        56,791,584\n",
       "│    └─BasicLayer: 2                               --                        --\n",
       "│    │    └─ModuleList: 3-4                        --                        25,203,264\n",
       "├─PatchEmbed: 1-1                                  [8, 3136, 128]            --\n",
       "│    └─Conv2d: 2-1                                 [8, 128, 56, 56]          6,272\n",
       "│    └─LayerNorm: 2-2                              [8, 3136, 128]            256\n",
       "├─Dropout: 1-2                                     [8, 3136, 128]            --\n",
       "├─Sequential: 1-3                                  [8, 49, 1024]             --\n",
       "│    └─BasicLayer: 2-3                             [8, 784, 256]             --\n",
       "│    │    └─PatchMerging: 3-5                      [8, 784, 256]             132,096\n",
       "│    └─BasicLayer: 2-4                             [8, 196, 512]             --\n",
       "│    │    └─PatchMerging: 3-6                      [8, 196, 512]             526,336\n",
       "│    └─BasicLayer: 2-5                             [8, 49, 1024]             --\n",
       "│    │    └─PatchMerging: 3-7                      [8, 49, 1024]             2,101,248\n",
       "│    └─BasicLayer: 2-6                             [8, 49, 1024]             --\n",
       "├─LayerNorm: 1-4                                   [8, 49, 1024]             2,048\n",
       "├─AdaptiveAvgPool1d: 1-5                           [8, 1024, 1]              --\n",
       "├─Linear: 1-6                                      [8, 1000]                 1,025,000\n",
       "====================================================================================================\n",
       "Total params: 87,704,680\n",
       "Trainable params: 87,704,680\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.44\n",
       "====================================================================================================\n",
       "Input size (MB): 4.82\n",
       "Forward/backward pass size (MB): 2312.17\n",
       "Params size (MB): 350.82\n",
       "Estimated Total Size (MB): 2667.81\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size = (8, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PatchEmbed(\n",
       "   (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "   (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Sequential(\n",
       "   (0): BasicLayer(\n",
       "     dim=128, input_resolution=(56, 56), depth=2\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): Identity()\n",
       "         (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (downsample): PatchMerging(\n",
       "       input_resolution=(56, 56), dim=128\n",
       "       (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "       (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicLayer(\n",
       "     dim=256, input_resolution=(28, 28), depth=2\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (downsample): PatchMerging(\n",
       "       input_resolution=(28, 28), dim=256\n",
       "       (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "       (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (2): BasicLayer(\n",
       "     dim=512, input_resolution=(14, 14), depth=18\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (2): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (3): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (4): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (5): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (6): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (7): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (8): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (9): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (10): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (11): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (12): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (13): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (14): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (15): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (16): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (17): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (downsample): PatchMerging(\n",
       "       input_resolution=(14, 14), dim=512\n",
       "       (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "       (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (3): BasicLayer(\n",
       "     dim=1024, input_resolution=(7, 7), depth=2\n",
       "     (blocks): ModuleList(\n",
       "       (0): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "       (1): SwinTransformerBlock(\n",
       "         (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (attn): WindowAttention(\n",
       "           (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "           (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "           (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "           (softmax): Softmax(dim=-1)\n",
       "         )\n",
       "         (drop_path): DropPath()\n",
       "         (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): Mlp(\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (act): GELU()\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (drop): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " LayerNorm((1024,), eps=1e-05, elementwise_affine=True),\n",
       " AdaptiveAvgPool1d(output_size=1),\n",
       " Linear(in_features=1024, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timm.models.swin_transformer.SwinTransformerBlock"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(list(list(model.children())[2][3].children())[0].children())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "patch_embed\n",
      "patch_embed.proj\n",
      "patch_embed.norm\n",
      "pos_drop\n",
      "layers\n",
      "layers.0\n",
      "layers.0.blocks\n",
      "layers.0.blocks.0\n",
      "layers.0.blocks.0.norm1\n",
      "layers.0.blocks.0.attn\n",
      "layers.0.blocks.0.attn.qkv\n",
      "layers.0.blocks.0.attn.attn_drop\n",
      "layers.0.blocks.0.attn.proj\n",
      "layers.0.blocks.0.attn.proj_drop\n",
      "layers.0.blocks.0.attn.softmax\n",
      "layers.0.blocks.0.drop_path\n",
      "layers.0.blocks.0.norm2\n",
      "layers.0.blocks.0.mlp\n",
      "layers.0.blocks.0.mlp.fc1\n",
      "layers.0.blocks.0.mlp.act\n",
      "layers.0.blocks.0.mlp.fc2\n",
      "layers.0.blocks.0.mlp.drop\n",
      "layers.0.blocks.1\n",
      "layers.0.blocks.1.norm1\n",
      "layers.0.blocks.1.attn\n",
      "layers.0.blocks.1.attn.qkv\n",
      "layers.0.blocks.1.attn.attn_drop\n",
      "layers.0.blocks.1.attn.proj\n",
      "layers.0.blocks.1.attn.proj_drop\n",
      "layers.0.blocks.1.attn.softmax\n",
      "layers.0.blocks.1.drop_path\n",
      "layers.0.blocks.1.norm2\n",
      "layers.0.blocks.1.mlp\n",
      "layers.0.blocks.1.mlp.fc1\n",
      "layers.0.blocks.1.mlp.act\n",
      "layers.0.blocks.1.mlp.fc2\n",
      "layers.0.blocks.1.mlp.drop\n",
      "layers.0.downsample\n",
      "layers.0.downsample.reduction\n",
      "layers.0.downsample.norm\n",
      "layers.1\n",
      "layers.1.blocks\n",
      "layers.1.blocks.0\n",
      "layers.1.blocks.0.norm1\n",
      "layers.1.blocks.0.attn\n",
      "layers.1.blocks.0.attn.qkv\n",
      "layers.1.blocks.0.attn.attn_drop\n",
      "layers.1.blocks.0.attn.proj\n",
      "layers.1.blocks.0.attn.proj_drop\n",
      "layers.1.blocks.0.attn.softmax\n",
      "layers.1.blocks.0.drop_path\n",
      "layers.1.blocks.0.norm2\n",
      "layers.1.blocks.0.mlp\n",
      "layers.1.blocks.0.mlp.fc1\n",
      "layers.1.blocks.0.mlp.act\n",
      "layers.1.blocks.0.mlp.fc2\n",
      "layers.1.blocks.0.mlp.drop\n",
      "layers.1.blocks.1\n",
      "layers.1.blocks.1.norm1\n",
      "layers.1.blocks.1.attn\n",
      "layers.1.blocks.1.attn.qkv\n",
      "layers.1.blocks.1.attn.attn_drop\n",
      "layers.1.blocks.1.attn.proj\n",
      "layers.1.blocks.1.attn.proj_drop\n",
      "layers.1.blocks.1.attn.softmax\n",
      "layers.1.blocks.1.drop_path\n",
      "layers.1.blocks.1.norm2\n",
      "layers.1.blocks.1.mlp\n",
      "layers.1.blocks.1.mlp.fc1\n",
      "layers.1.blocks.1.mlp.act\n",
      "layers.1.blocks.1.mlp.fc2\n",
      "layers.1.blocks.1.mlp.drop\n",
      "layers.1.downsample\n",
      "layers.1.downsample.reduction\n",
      "layers.1.downsample.norm\n",
      "layers.2\n",
      "layers.2.blocks\n",
      "layers.2.blocks.0\n",
      "layers.2.blocks.0.norm1\n",
      "layers.2.blocks.0.attn\n",
      "layers.2.blocks.0.attn.qkv\n",
      "layers.2.blocks.0.attn.attn_drop\n",
      "layers.2.blocks.0.attn.proj\n",
      "layers.2.blocks.0.attn.proj_drop\n",
      "layers.2.blocks.0.attn.softmax\n",
      "layers.2.blocks.0.drop_path\n",
      "layers.2.blocks.0.norm2\n",
      "layers.2.blocks.0.mlp\n",
      "layers.2.blocks.0.mlp.fc1\n",
      "layers.2.blocks.0.mlp.act\n",
      "layers.2.blocks.0.mlp.fc2\n",
      "layers.2.blocks.0.mlp.drop\n",
      "layers.2.blocks.1\n",
      "layers.2.blocks.1.norm1\n",
      "layers.2.blocks.1.attn\n",
      "layers.2.blocks.1.attn.qkv\n",
      "layers.2.blocks.1.attn.attn_drop\n",
      "layers.2.blocks.1.attn.proj\n",
      "layers.2.blocks.1.attn.proj_drop\n",
      "layers.2.blocks.1.attn.softmax\n",
      "layers.2.blocks.1.drop_path\n",
      "layers.2.blocks.1.norm2\n",
      "layers.2.blocks.1.mlp\n",
      "layers.2.blocks.1.mlp.fc1\n",
      "layers.2.blocks.1.mlp.act\n",
      "layers.2.blocks.1.mlp.fc2\n",
      "layers.2.blocks.1.mlp.drop\n",
      "layers.2.blocks.2\n",
      "layers.2.blocks.2.norm1\n",
      "layers.2.blocks.2.attn\n",
      "layers.2.blocks.2.attn.qkv\n",
      "layers.2.blocks.2.attn.attn_drop\n",
      "layers.2.blocks.2.attn.proj\n",
      "layers.2.blocks.2.attn.proj_drop\n",
      "layers.2.blocks.2.attn.softmax\n",
      "layers.2.blocks.2.drop_path\n",
      "layers.2.blocks.2.norm2\n",
      "layers.2.blocks.2.mlp\n",
      "layers.2.blocks.2.mlp.fc1\n",
      "layers.2.blocks.2.mlp.act\n",
      "layers.2.blocks.2.mlp.fc2\n",
      "layers.2.blocks.2.mlp.drop\n",
      "layers.2.blocks.3\n",
      "layers.2.blocks.3.norm1\n",
      "layers.2.blocks.3.attn\n",
      "layers.2.blocks.3.attn.qkv\n",
      "layers.2.blocks.3.attn.attn_drop\n",
      "layers.2.blocks.3.attn.proj\n",
      "layers.2.blocks.3.attn.proj_drop\n",
      "layers.2.blocks.3.attn.softmax\n",
      "layers.2.blocks.3.drop_path\n",
      "layers.2.blocks.3.norm2\n",
      "layers.2.blocks.3.mlp\n",
      "layers.2.blocks.3.mlp.fc1\n",
      "layers.2.blocks.3.mlp.act\n",
      "layers.2.blocks.3.mlp.fc2\n",
      "layers.2.blocks.3.mlp.drop\n",
      "layers.2.blocks.4\n",
      "layers.2.blocks.4.norm1\n",
      "layers.2.blocks.4.attn\n",
      "layers.2.blocks.4.attn.qkv\n",
      "layers.2.blocks.4.attn.attn_drop\n",
      "layers.2.blocks.4.attn.proj\n",
      "layers.2.blocks.4.attn.proj_drop\n",
      "layers.2.blocks.4.attn.softmax\n",
      "layers.2.blocks.4.drop_path\n",
      "layers.2.blocks.4.norm2\n",
      "layers.2.blocks.4.mlp\n",
      "layers.2.blocks.4.mlp.fc1\n",
      "layers.2.blocks.4.mlp.act\n",
      "layers.2.blocks.4.mlp.fc2\n",
      "layers.2.blocks.4.mlp.drop\n",
      "layers.2.blocks.5\n",
      "layers.2.blocks.5.norm1\n",
      "layers.2.blocks.5.attn\n",
      "layers.2.blocks.5.attn.qkv\n",
      "layers.2.blocks.5.attn.attn_drop\n",
      "layers.2.blocks.5.attn.proj\n",
      "layers.2.blocks.5.attn.proj_drop\n",
      "layers.2.blocks.5.attn.softmax\n",
      "layers.2.blocks.5.drop_path\n",
      "layers.2.blocks.5.norm2\n",
      "layers.2.blocks.5.mlp\n",
      "layers.2.blocks.5.mlp.fc1\n",
      "layers.2.blocks.5.mlp.act\n",
      "layers.2.blocks.5.mlp.fc2\n",
      "layers.2.blocks.5.mlp.drop\n",
      "layers.2.blocks.6\n",
      "layers.2.blocks.6.norm1\n",
      "layers.2.blocks.6.attn\n",
      "layers.2.blocks.6.attn.qkv\n",
      "layers.2.blocks.6.attn.attn_drop\n",
      "layers.2.blocks.6.attn.proj\n",
      "layers.2.blocks.6.attn.proj_drop\n",
      "layers.2.blocks.6.attn.softmax\n",
      "layers.2.blocks.6.drop_path\n",
      "layers.2.blocks.6.norm2\n",
      "layers.2.blocks.6.mlp\n",
      "layers.2.blocks.6.mlp.fc1\n",
      "layers.2.blocks.6.mlp.act\n",
      "layers.2.blocks.6.mlp.fc2\n",
      "layers.2.blocks.6.mlp.drop\n",
      "layers.2.blocks.7\n",
      "layers.2.blocks.7.norm1\n",
      "layers.2.blocks.7.attn\n",
      "layers.2.blocks.7.attn.qkv\n",
      "layers.2.blocks.7.attn.attn_drop\n",
      "layers.2.blocks.7.attn.proj\n",
      "layers.2.blocks.7.attn.proj_drop\n",
      "layers.2.blocks.7.attn.softmax\n",
      "layers.2.blocks.7.drop_path\n",
      "layers.2.blocks.7.norm2\n",
      "layers.2.blocks.7.mlp\n",
      "layers.2.blocks.7.mlp.fc1\n",
      "layers.2.blocks.7.mlp.act\n",
      "layers.2.blocks.7.mlp.fc2\n",
      "layers.2.blocks.7.mlp.drop\n",
      "layers.2.blocks.8\n",
      "layers.2.blocks.8.norm1\n",
      "layers.2.blocks.8.attn\n",
      "layers.2.blocks.8.attn.qkv\n",
      "layers.2.blocks.8.attn.attn_drop\n",
      "layers.2.blocks.8.attn.proj\n",
      "layers.2.blocks.8.attn.proj_drop\n",
      "layers.2.blocks.8.attn.softmax\n",
      "layers.2.blocks.8.drop_path\n",
      "layers.2.blocks.8.norm2\n",
      "layers.2.blocks.8.mlp\n",
      "layers.2.blocks.8.mlp.fc1\n",
      "layers.2.blocks.8.mlp.act\n",
      "layers.2.blocks.8.mlp.fc2\n",
      "layers.2.blocks.8.mlp.drop\n",
      "layers.2.blocks.9\n",
      "layers.2.blocks.9.norm1\n",
      "layers.2.blocks.9.attn\n",
      "layers.2.blocks.9.attn.qkv\n",
      "layers.2.blocks.9.attn.attn_drop\n",
      "layers.2.blocks.9.attn.proj\n",
      "layers.2.blocks.9.attn.proj_drop\n",
      "layers.2.blocks.9.attn.softmax\n",
      "layers.2.blocks.9.drop_path\n",
      "layers.2.blocks.9.norm2\n",
      "layers.2.blocks.9.mlp\n",
      "layers.2.blocks.9.mlp.fc1\n",
      "layers.2.blocks.9.mlp.act\n",
      "layers.2.blocks.9.mlp.fc2\n",
      "layers.2.blocks.9.mlp.drop\n",
      "layers.2.blocks.10\n",
      "layers.2.blocks.10.norm1\n",
      "layers.2.blocks.10.attn\n",
      "layers.2.blocks.10.attn.qkv\n",
      "layers.2.blocks.10.attn.attn_drop\n",
      "layers.2.blocks.10.attn.proj\n",
      "layers.2.blocks.10.attn.proj_drop\n",
      "layers.2.blocks.10.attn.softmax\n",
      "layers.2.blocks.10.drop_path\n",
      "layers.2.blocks.10.norm2\n",
      "layers.2.blocks.10.mlp\n",
      "layers.2.blocks.10.mlp.fc1\n",
      "layers.2.blocks.10.mlp.act\n",
      "layers.2.blocks.10.mlp.fc2\n",
      "layers.2.blocks.10.mlp.drop\n",
      "layers.2.blocks.11\n",
      "layers.2.blocks.11.norm1\n",
      "layers.2.blocks.11.attn\n",
      "layers.2.blocks.11.attn.qkv\n",
      "layers.2.blocks.11.attn.attn_drop\n",
      "layers.2.blocks.11.attn.proj\n",
      "layers.2.blocks.11.attn.proj_drop\n",
      "layers.2.blocks.11.attn.softmax\n",
      "layers.2.blocks.11.drop_path\n",
      "layers.2.blocks.11.norm2\n",
      "layers.2.blocks.11.mlp\n",
      "layers.2.blocks.11.mlp.fc1\n",
      "layers.2.blocks.11.mlp.act\n",
      "layers.2.blocks.11.mlp.fc2\n",
      "layers.2.blocks.11.mlp.drop\n",
      "layers.2.blocks.12\n",
      "layers.2.blocks.12.norm1\n",
      "layers.2.blocks.12.attn\n",
      "layers.2.blocks.12.attn.qkv\n",
      "layers.2.blocks.12.attn.attn_drop\n",
      "layers.2.blocks.12.attn.proj\n",
      "layers.2.blocks.12.attn.proj_drop\n",
      "layers.2.blocks.12.attn.softmax\n",
      "layers.2.blocks.12.drop_path\n",
      "layers.2.blocks.12.norm2\n",
      "layers.2.blocks.12.mlp\n",
      "layers.2.blocks.12.mlp.fc1\n",
      "layers.2.blocks.12.mlp.act\n",
      "layers.2.blocks.12.mlp.fc2\n",
      "layers.2.blocks.12.mlp.drop\n",
      "layers.2.blocks.13\n",
      "layers.2.blocks.13.norm1\n",
      "layers.2.blocks.13.attn\n",
      "layers.2.blocks.13.attn.qkv\n",
      "layers.2.blocks.13.attn.attn_drop\n",
      "layers.2.blocks.13.attn.proj\n",
      "layers.2.blocks.13.attn.proj_drop\n",
      "layers.2.blocks.13.attn.softmax\n",
      "layers.2.blocks.13.drop_path\n",
      "layers.2.blocks.13.norm2\n",
      "layers.2.blocks.13.mlp\n",
      "layers.2.blocks.13.mlp.fc1\n",
      "layers.2.blocks.13.mlp.act\n",
      "layers.2.blocks.13.mlp.fc2\n",
      "layers.2.blocks.13.mlp.drop\n",
      "layers.2.blocks.14\n",
      "layers.2.blocks.14.norm1\n",
      "layers.2.blocks.14.attn\n",
      "layers.2.blocks.14.attn.qkv\n",
      "layers.2.blocks.14.attn.attn_drop\n",
      "layers.2.blocks.14.attn.proj\n",
      "layers.2.blocks.14.attn.proj_drop\n",
      "layers.2.blocks.14.attn.softmax\n",
      "layers.2.blocks.14.drop_path\n",
      "layers.2.blocks.14.norm2\n",
      "layers.2.blocks.14.mlp\n",
      "layers.2.blocks.14.mlp.fc1\n",
      "layers.2.blocks.14.mlp.act\n",
      "layers.2.blocks.14.mlp.fc2\n",
      "layers.2.blocks.14.mlp.drop\n",
      "layers.2.blocks.15\n",
      "layers.2.blocks.15.norm1\n",
      "layers.2.blocks.15.attn\n",
      "layers.2.blocks.15.attn.qkv\n",
      "layers.2.blocks.15.attn.attn_drop\n",
      "layers.2.blocks.15.attn.proj\n",
      "layers.2.blocks.15.attn.proj_drop\n",
      "layers.2.blocks.15.attn.softmax\n",
      "layers.2.blocks.15.drop_path\n",
      "layers.2.blocks.15.norm2\n",
      "layers.2.blocks.15.mlp\n",
      "layers.2.blocks.15.mlp.fc1\n",
      "layers.2.blocks.15.mlp.act\n",
      "layers.2.blocks.15.mlp.fc2\n",
      "layers.2.blocks.15.mlp.drop\n",
      "layers.2.blocks.16\n",
      "layers.2.blocks.16.norm1\n",
      "layers.2.blocks.16.attn\n",
      "layers.2.blocks.16.attn.qkv\n",
      "layers.2.blocks.16.attn.attn_drop\n",
      "layers.2.blocks.16.attn.proj\n",
      "layers.2.blocks.16.attn.proj_drop\n",
      "layers.2.blocks.16.attn.softmax\n",
      "layers.2.blocks.16.drop_path\n",
      "layers.2.blocks.16.norm2\n",
      "layers.2.blocks.16.mlp\n",
      "layers.2.blocks.16.mlp.fc1\n",
      "layers.2.blocks.16.mlp.act\n",
      "layers.2.blocks.16.mlp.fc2\n",
      "layers.2.blocks.16.mlp.drop\n",
      "layers.2.blocks.17\n",
      "layers.2.blocks.17.norm1\n",
      "layers.2.blocks.17.attn\n",
      "layers.2.blocks.17.attn.qkv\n",
      "layers.2.blocks.17.attn.attn_drop\n",
      "layers.2.blocks.17.attn.proj\n",
      "layers.2.blocks.17.attn.proj_drop\n",
      "layers.2.blocks.17.attn.softmax\n",
      "layers.2.blocks.17.drop_path\n",
      "layers.2.blocks.17.norm2\n",
      "layers.2.blocks.17.mlp\n",
      "layers.2.blocks.17.mlp.fc1\n",
      "layers.2.blocks.17.mlp.act\n",
      "layers.2.blocks.17.mlp.fc2\n",
      "layers.2.blocks.17.mlp.drop\n",
      "layers.2.downsample\n",
      "layers.2.downsample.reduction\n",
      "layers.2.downsample.norm\n",
      "layers.3\n",
      "layers.3.blocks\n",
      "layers.3.blocks.0\n",
      "layers.3.blocks.0.norm1\n",
      "layers.3.blocks.0.attn\n",
      "layers.3.blocks.0.attn.qkv\n",
      "layers.3.blocks.0.attn.attn_drop\n",
      "layers.3.blocks.0.attn.proj\n",
      "layers.3.blocks.0.attn.proj_drop\n",
      "layers.3.blocks.0.attn.softmax\n",
      "layers.3.blocks.0.drop_path\n",
      "layers.3.blocks.0.norm2\n",
      "layers.3.blocks.0.mlp\n",
      "layers.3.blocks.0.mlp.fc1\n",
      "layers.3.blocks.0.mlp.act\n",
      "layers.3.blocks.0.mlp.fc2\n",
      "layers.3.blocks.0.mlp.drop\n",
      "layers.3.blocks.1\n",
      "layers.3.blocks.1.norm1\n",
      "layers.3.blocks.1.attn\n",
      "layers.3.blocks.1.attn.qkv\n",
      "layers.3.blocks.1.attn.attn_drop\n",
      "layers.3.blocks.1.attn.proj\n",
      "layers.3.blocks.1.attn.proj_drop\n",
      "layers.3.blocks.1.attn.softmax\n",
      "layers.3.blocks.1.drop_path\n",
      "layers.3.blocks.1.norm2\n",
      "layers.3.blocks.1.mlp\n",
      "layers.3.blocks.1.mlp.fc1\n",
      "layers.3.blocks.1.mlp.act\n",
      "layers.3.blocks.1.mlp.fc2\n",
      "layers.3.blocks.1.mlp.drop\n",
      "norm\n",
      "avgpool\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/deit/archive/main.zip\" to /home/pio/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /home/pio/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "deit = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "deit.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "patch_embed\n",
      "patch_embed.proj\n",
      "patch_embed.norm\n",
      "pos_drop\n",
      "blocks\n",
      "blocks.0\n",
      "blocks.0.norm1\n",
      "blocks.0.attn\n",
      "blocks.0.attn.qkv\n",
      "blocks.0.attn.attn_drop\n",
      "blocks.0.attn.proj\n",
      "blocks.0.attn.proj_drop\n",
      "blocks.0.drop_path\n",
      "blocks.0.norm2\n",
      "blocks.0.mlp\n",
      "blocks.0.mlp.fc1\n",
      "blocks.0.mlp.act\n",
      "blocks.0.mlp.fc2\n",
      "blocks.0.mlp.drop\n",
      "blocks.1\n",
      "blocks.1.norm1\n",
      "blocks.1.attn\n",
      "blocks.1.attn.qkv\n",
      "blocks.1.attn.attn_drop\n",
      "blocks.1.attn.proj\n",
      "blocks.1.attn.proj_drop\n",
      "blocks.1.drop_path\n",
      "blocks.1.norm2\n",
      "blocks.1.mlp\n",
      "blocks.1.mlp.fc1\n",
      "blocks.1.mlp.act\n",
      "blocks.1.mlp.fc2\n",
      "blocks.1.mlp.drop\n",
      "blocks.2\n",
      "blocks.2.norm1\n",
      "blocks.2.attn\n",
      "blocks.2.attn.qkv\n",
      "blocks.2.attn.attn_drop\n",
      "blocks.2.attn.proj\n",
      "blocks.2.attn.proj_drop\n",
      "blocks.2.drop_path\n",
      "blocks.2.norm2\n",
      "blocks.2.mlp\n",
      "blocks.2.mlp.fc1\n",
      "blocks.2.mlp.act\n",
      "blocks.2.mlp.fc2\n",
      "blocks.2.mlp.drop\n",
      "blocks.3\n",
      "blocks.3.norm1\n",
      "blocks.3.attn\n",
      "blocks.3.attn.qkv\n",
      "blocks.3.attn.attn_drop\n",
      "blocks.3.attn.proj\n",
      "blocks.3.attn.proj_drop\n",
      "blocks.3.drop_path\n",
      "blocks.3.norm2\n",
      "blocks.3.mlp\n",
      "blocks.3.mlp.fc1\n",
      "blocks.3.mlp.act\n",
      "blocks.3.mlp.fc2\n",
      "blocks.3.mlp.drop\n",
      "blocks.4\n",
      "blocks.4.norm1\n",
      "blocks.4.attn\n",
      "blocks.4.attn.qkv\n",
      "blocks.4.attn.attn_drop\n",
      "blocks.4.attn.proj\n",
      "blocks.4.attn.proj_drop\n",
      "blocks.4.drop_path\n",
      "blocks.4.norm2\n",
      "blocks.4.mlp\n",
      "blocks.4.mlp.fc1\n",
      "blocks.4.mlp.act\n",
      "blocks.4.mlp.fc2\n",
      "blocks.4.mlp.drop\n",
      "blocks.5\n",
      "blocks.5.norm1\n",
      "blocks.5.attn\n",
      "blocks.5.attn.qkv\n",
      "blocks.5.attn.attn_drop\n",
      "blocks.5.attn.proj\n",
      "blocks.5.attn.proj_drop\n",
      "blocks.5.drop_path\n",
      "blocks.5.norm2\n",
      "blocks.5.mlp\n",
      "blocks.5.mlp.fc1\n",
      "blocks.5.mlp.act\n",
      "blocks.5.mlp.fc2\n",
      "blocks.5.mlp.drop\n",
      "blocks.6\n",
      "blocks.6.norm1\n",
      "blocks.6.attn\n",
      "blocks.6.attn.qkv\n",
      "blocks.6.attn.attn_drop\n",
      "blocks.6.attn.proj\n",
      "blocks.6.attn.proj_drop\n",
      "blocks.6.drop_path\n",
      "blocks.6.norm2\n",
      "blocks.6.mlp\n",
      "blocks.6.mlp.fc1\n",
      "blocks.6.mlp.act\n",
      "blocks.6.mlp.fc2\n",
      "blocks.6.mlp.drop\n",
      "blocks.7\n",
      "blocks.7.norm1\n",
      "blocks.7.attn\n",
      "blocks.7.attn.qkv\n",
      "blocks.7.attn.attn_drop\n",
      "blocks.7.attn.proj\n",
      "blocks.7.attn.proj_drop\n",
      "blocks.7.drop_path\n",
      "blocks.7.norm2\n",
      "blocks.7.mlp\n",
      "blocks.7.mlp.fc1\n",
      "blocks.7.mlp.act\n",
      "blocks.7.mlp.fc2\n",
      "blocks.7.mlp.drop\n",
      "blocks.8\n",
      "blocks.8.norm1\n",
      "blocks.8.attn\n",
      "blocks.8.attn.qkv\n",
      "blocks.8.attn.attn_drop\n",
      "blocks.8.attn.proj\n",
      "blocks.8.attn.proj_drop\n",
      "blocks.8.drop_path\n",
      "blocks.8.norm2\n",
      "blocks.8.mlp\n",
      "blocks.8.mlp.fc1\n",
      "blocks.8.mlp.act\n",
      "blocks.8.mlp.fc2\n",
      "blocks.8.mlp.drop\n",
      "blocks.9\n",
      "blocks.9.norm1\n",
      "blocks.9.attn\n",
      "blocks.9.attn.qkv\n",
      "blocks.9.attn.attn_drop\n",
      "blocks.9.attn.proj\n",
      "blocks.9.attn.proj_drop\n",
      "blocks.9.drop_path\n",
      "blocks.9.norm2\n",
      "blocks.9.mlp\n",
      "blocks.9.mlp.fc1\n",
      "blocks.9.mlp.act\n",
      "blocks.9.mlp.fc2\n",
      "blocks.9.mlp.drop\n",
      "blocks.10\n",
      "blocks.10.norm1\n",
      "blocks.10.attn\n",
      "blocks.10.attn.qkv\n",
      "blocks.10.attn.attn_drop\n",
      "blocks.10.attn.proj\n",
      "blocks.10.attn.proj_drop\n",
      "blocks.10.drop_path\n",
      "blocks.10.norm2\n",
      "blocks.10.mlp\n",
      "blocks.10.mlp.fc1\n",
      "blocks.10.mlp.act\n",
      "blocks.10.mlp.fc2\n",
      "blocks.10.mlp.drop\n",
      "blocks.11\n",
      "blocks.11.norm1\n",
      "blocks.11.attn\n",
      "blocks.11.attn.qkv\n",
      "blocks.11.attn.attn_drop\n",
      "blocks.11.attn.proj\n",
      "blocks.11.attn.proj_drop\n",
      "blocks.11.drop_path\n",
      "blocks.11.norm2\n",
      "blocks.11.mlp\n",
      "blocks.11.mlp.fc1\n",
      "blocks.11.mlp.act\n",
      "blocks.11.mlp.fc2\n",
      "blocks.11.mlp.drop\n",
      "norm\n",
      "pre_logits\n",
      "head\n"
     ]
    }
   ],
   "source": [
    "for name, module in deit.named_modules():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0b6b354958d59b4890170593aabca2d81d77c2a3806ff7f39955507ba6dfc558"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
